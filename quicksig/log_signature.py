import jax
from quicksig.batch_ops import batch_tensor_log
from quicksig.path_signature import batch_signature
from enum import StrEnum

from collections import defaultdict
import jax.numpy as jnp


class LogSignatureType(StrEnum):
    EXPANDED = "expanded"  # $$ \in T(V) $$
    LYNDON = "lyndon"


def batch_log_signature(path: jax.Array, depth: int, log_signature_type: LogSignatureType) -> list[jax.Array]:
    n_features = path.shape[-1]
    signature: list[jax.Array] = batch_signature(path, depth, stream=False)
    match log_signature_type:
        case LogSignatureType.EXPANDED:
            return batch_tensor_log(signature, n_features)
        case LogSignatureType.LYNDON:
            indices = duval_algorithm(depth, n_features)
            log_signature = batch_tensor_log(signature, n_features)
            return compress(log_signature, indices)
        case _:
            raise ValueError(f"Invalid log signature type: {log_signature_type}")


def duval_algorithm(depth: int, dim: int) -> list[jax.Array]:
    list_of_words = defaultdict(list)
    word = [-1]
    while word:
        word[-1] += 1
        m = len(word)
        # yield word
        list_of_words[m - 1].append(jnp.array(word))
        while len(word) < depth:
            word.append(word[-m])
        while word and word[-1] == dim - 1:
            word.pop()

    return [jnp.stack(list_of_words[i]) for i in range(depth)]


def index_select(input: jax.Array, indices: jax.Array) -> jax.Array:
    """
    Select entries in m-level tensor based on given indices
    This function will help compressing log-signatures

    Args:
        input: size (dim, dim, ..., dim)
        indices: size (dim, n)
    Return:
        A 1D jnp.ndarray
    """

    # Handle scalar input: if log-signature term is scalar (e.g., 0.0 for higher orders),its projection onto Lyndon basis is all zeros.
    if input.ndim == 0:
        return jnp.zeros(indices.shape[0], dtype=input.dtype)

    dim_first_axis = input.shape[0]
    ndim_input_tensor = input.ndim
    n_components_in_indices = indices.shape[1]  # Number of components in each Lyndon word (row of indices)

    # If Lyndon words have more components than the tensor has dimensions,
    # the tensor is degenerate w.r.t this basis. All projections are zero.
    if n_components_in_indices > ndim_input_tensor:
        return jnp.zeros(indices.shape[0], dtype=input.dtype)

    # Flatten matrix A in Fortran-style
    flattened = input.ravel("F")

    # Strides for Fortran-style indexing.
    # Assumes a tensor of rank `n_components_in_indices` where each dimension's size is `dim_first_axis`.
    # This is appropriate if n_components_in_indices == ndim_input_tensor.
    # If n_components_in_indices < ndim_input_tensor (e.g., 2-component words for a 3D tensor),
    # this selects from the "initial part" of the tensor, which is valid.
    strides_array = jnp.array([dim_first_axis**i for i in range(n_components_in_indices)])

    def _select(one_index_row: jax.Array) -> jax.Array:
        """one_index_row is a 1D jnp.ndarray (a single Lyndon word)"""
        # This computes the flat index for Fortran-style raveled arrays
        position = jnp.sum(one_index_row * strides_array)
        return flattened[position]

    return jax.vmap(_select)(indices)


def compress(expanded_batch: list[jax.Array], lyndon_indices: list[jax.Array]) -> list[jax.Array]:
    """
    Compress expanded log-signatures using Lyndon words, handling batches.

    Args:
        expanded_batch: List of `jnp.ndarray`. Each element is a batched tensor,
                        e.g., expanded_batch[k] has shape (batch_size, n_features, ..., n_features).
        lyndon_indices: List of `jnp.ndarray` generated by Lyndon words. Each element
                        lyndon_indices[k] has shape (num_lyndon_words_at_level_k, k+1).
                        These are not batched.

    Returns:
        A list of compressed `jnp.ndarray`. Each element will have shape
        (batch_size, num_lyndon_words_at_level_k).
    """

    # vmap index_select over the batch dimension of the input tensor.
    # - The first argument to index_select (input tensor) is mapped along axis 0.
    # - The second argument to index_select (Lyndon indices) is not mapped (it's broadcasted).
    # The output will also have a batch dimension at axis 0.
    vmap_index_select_over_batch = jax.vmap(index_select, in_axes=(0, None), out_axes=0)

    result_batch_compressed = []
    for term_batch, term_lyndon_indices in zip(expanded_batch, lyndon_indices):
        # term_batch shape: (batch_size, n_features, ..., n_features)
        # term_lyndon_indices shape: (num_lyndon_words, num_components)

        # Apply vmapped index_select
        compressed_term_for_batch = vmap_index_select_over_batch(term_batch, term_lyndon_indices)
        # compressed_term_for_batch shape: (batch_size, num_lyndon_words)
        result_batch_compressed.append(compressed_term_for_batch)

    return result_batch_compressed
